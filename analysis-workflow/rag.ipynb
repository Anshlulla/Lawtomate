{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"data.txt\"\n",
    "loader = TextLoader(file_path=file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13276\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001F409F1C6B0>, search_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persist_directory = r\"C:\\Users\\Ansh Lulla\\VS-Code\\sam\\chroma-db\"\n",
    "vectorstore = Chroma.from_documents(documents=chunks, \n",
    "                                    embedding=embedding_model, \n",
    "                                    persist_directory=persist_directory)\n",
    "retriever = vectorstore.as_retriever()  \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    As a banking-focused assistant, your responses are strictly limited to the following topics:\n",
    "    - Card Management\n",
    "    - Transaction Inquiries\n",
    "    - Loans and Mortgages\n",
    "    - Transfers and Payments\n",
    "    - Customer Service\n",
    "    - Fees and Charges\n",
    "    - Identity Verification\n",
    "    - Account Management\n",
    "    - General Banking Support\n",
    "\n",
    "    ### Instructions for Responses:\n",
    "\n",
    "    1. **For Banking-Related Queries (Intent = \"banking\")**:\n",
    "    - Provide a complete and concise answer directly based on the context and question.\n",
    "    - Avoid reasoning, unnecessary details, or incomplete answers.\n",
    "    - End with: \"Is there anything else I can assist you with today?\"\n",
    "\n",
    "    2. **For Non-Banking Queries (Intent = \"nonBanking\")**:\n",
    "    - **Reply only with**: \"I'm here to assist with banking-related queries only. Could you please ask a question about topics like transactions, card management, or loans?\"\n",
    "\n",
    "    Intent: {intent}\n",
    "    Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F41A087E60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F41A084BC0>, model_name='Gemma2-9b-It', temperature=0.6, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(model=\"Gemma2-9b-It\", temperature=0.6, api_key=api_key)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input', 'intent'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'intent'], input_types={}, partial_variables={}, template='\\n    As a banking-focused assistant, your responses are strictly limited to the following topics:\\n    - Card Management\\n    - Transaction Inquiries\\n    - Loans and Mortgages\\n    - Transfers and Payments\\n    - Customer Service\\n    - Fees and Charges\\n    - Identity Verification\\n    - Account Management\\n    - General Banking Support\\n\\n    ### Instructions for Responses:\\n\\n    1. **For Banking-Related Queries (Intent = \"banking\")**:\\n    - Provide a complete and concise answer directly based on the context and question.\\n    - Avoid reasoning, unnecessary details, or incomplete answers.\\n    - End with: \"Is there anything else I can assist you with today?\"\\n\\n    2. **For Non-Banking Queries (Intent = \"nonBanking\")**:\\n    - **Reply only with**: \"I\\'m here to assist with banking-related queries only. Could you please ask a question about topics like transactions, card management, or loans?\"\\n\\n    Intent: {intent}\\n    Context: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F41A087E60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F41A084BC0>, model_name='Gemma2-9b-It', temperature=0.6, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001F409F1C6B0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input', 'intent'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'intent'], input_types={}, partial_variables={}, template='\\n    As a banking-focused assistant, your responses are strictly limited to the following topics:\\n    - Card Management\\n    - Transaction Inquiries\\n    - Loans and Mortgages\\n    - Transfers and Payments\\n    - Customer Service\\n    - Fees and Charges\\n    - Identity Verification\\n    - Account Management\\n    - General Banking Support\\n\\n    ### Instructions for Responses:\\n\\n    1. **For Banking-Related Queries (Intent = \"banking\")**:\\n    - Provide a complete and concise answer directly based on the context and question.\\n    - Avoid reasoning, unnecessary details, or incomplete answers.\\n    - End with: \"Is there anything else I can assist you with today?\"\\n\\n    2. **For Non-Banking Queries (Intent = \"nonBanking\")**:\\n    - **Reply only with**: \"I\\'m here to assist with banking-related queries only. Could you please ask a question about topics like transactions, card management, or loans?\"\\n\\n    Intent: {intent}\\n    Context: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F41A087E60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F41A084BC0>, model_name='Gemma2-9b-It', temperature=0.6, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=qa_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A loan is a sum of money borrowed from a lender with the agreement to repay it, usually with interest, over a specified period. \n",
      "\n",
      "Is there anything else I can assist you with today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is a loan?\", \"intent\": \"banking\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F45BBC1760>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n    Given a chat history and the latest user query, which might be a reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, reformulate it if needed otherwise return it as it is.    \\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualize_q_system_prompt = (\"\"\"\n",
    "    Given a chat history and the latest user query, which might be a reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, reformulate it if needed otherwise return it as it is.    \n",
    "\"\"\")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "contextualize_q_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001F409F1C6B0>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F45BBC1760>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n    Given a chat history and the latest user query, which might be a reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, reformulate it if needed otherwise return it as it is.    \\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F41A087E60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F41A084BC0>, model_name='Gemma2-9b-It', temperature=0.6, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001F409F1C6B0>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001F409F1C6B0>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F45BBC1760>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n    Given a chat history and the latest user query, which might be a reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, reformulate it if needed otherwise return it as it is.    \\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F41A087E60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F41A084BC0>, model_name='Gemma2-9b-It', temperature=0.6, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001F409F1C6B0>, search_kwargs={})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input', 'intent'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F45BBC1760>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'intent'], input_types={}, partial_variables={}, template='\\n    As a banking-focused assistant, your responses are strictly limited to the following topics:\\n    - Card Management\\n    - Transaction Inquiries\\n    - Loans and Mortgages\\n    - Transfers and Payments\\n    - Customer Service\\n    - Fees and Charges\\n    - Identity Verification\\n    - Account Management\\n    - General Banking Support\\n\\n    ### Instructions for Responses:\\n\\n    1. **For Banking-Related Queries (Intent = \"banking\")**:\\n    - Provide a complete and concise answer directly based on the context and question.\\n    - Avoid reasoning, unnecessary details, or incomplete answers.\\n    - End with: \"Is there anything else I can assist you with today?\"\\n\\n    2. **For Non-Banking Queries (Intent = \"nonBanking\")**:\\n    - **Reply only with**: \"I\\'m here to assist with banking-related queries only. Could you please ask a question about topics like transactions, card management, or loans?\"\\n\\n    Intent: {intent}\\n    Context: {context}\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F41A087E60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F41A084BC0>, model_name='Gemma2-9b-It', temperature=0.6, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual funds are investment vehicles that pool money from multiple investors to invest in a diversified portfolio of assets such as stocks, bonds, and other securities. They are managed by professional fund managers who make investment decisions on behalf of the investors. \n",
      "\n",
      "Mutual funds offer several benefits, including diversification, professional management, liquidity, and potential for growth. \n",
      "\n",
      "Is there anything else I can assist you with today?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------\n",
      "Mutual funds offer a way to invest in a diversified portfolio of assets without having to individually research and select each security. \n",
      "\n",
      "Here's a breakdown of some key benefits:\n",
      "\n",
      "* **Diversification:** By investing in a mutual fund, your money is spread across a variety of investments, which helps to reduce risk. \n",
      "* **Professional Management:**  Experienced fund managers make investment decisions based on market research and analysis, aiming to achieve the fund's investment objectives.\n",
      "* **Liquidity:**  Many mutual funds allow you to buy and sell shares easily, providing flexibility in managing your investments.\n",
      "* **Accessibility:** Mutual funds are generally available to investors with relatively small amounts of money, making them accessible to a wide range of individuals.\n",
      "\n",
      "Is there anything else I can assist you with today? \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What are mutual funds and why should I invest in them?\"\n",
    "response1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history, \"intent\": \"banking\"})\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response1[\"answer\"])\n",
    "])\n",
    "\n",
    "question2 = \"Tell me more about it.\"\n",
    "response2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history,\"intent\": \"banking\"})\n",
    "print(response1[\"answer\"])\n",
    "print(\"---------------------\")\n",
    "print(response2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many Indian banks offer mutual fund services. Some popular options include:\n",
      "\n",
      "* HDFC Bank\n",
      "* ICICI Bank\n",
      "* Axis Bank\n",
      "* SBI\n",
      "* Kotak Mahindra Bank\n",
      "\n",
      "It's recommended to research and compare different banks based on their:\n",
      "\n",
      "*  Fees\n",
      "*  Fund selection\n",
      "*  Investment platforms\n",
      "*  Customer service \n",
      "\n",
      "Is there anything else I can assist you with today? \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response2[\"answer\"])\n",
    "])\n",
    "\n",
    "question3 = \"Which Indian banks offer are good for it?\"\n",
    "response3 = rag_chain.invoke({\"input\": question3, \"chat_history\": chat_history,\"intent\": \"banking\"})\n",
    "print(response3[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
