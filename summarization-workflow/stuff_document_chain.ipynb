{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 0}, page_content='PlotPoint - An AI-Driven Framework for\\nAutomated Movie Scriptwriting and Scene\\nGeneration\\nAnsh Lulla, Aayush Koul, Rampalli Agni Mithra,\\nAniket K. Shahade ∗, Mayur Gaikwad, Shruti Patil\\nDepartment of AIML ,\\nSymbiosis Institute of Technology, Pune Campus , Symbiosis International (Deemed University), Pune, India\\n∗aniket.shahade@sitpune.edu.in\\nAbstract—Script writing has become a labor-intensive\\nprocess, especially when tailored to specific scene setting,\\nculture or character. Recent advancements in the field\\nof Natural Language Processing and Deep Learning\\nalgorithms have made it possible to automate the process\\nof script writing. With a simple prompt, it is possible to\\ngenerate the entire script for a movie, which would be\\ncustomizable by the input’s creativity and imagination.\\nRetrieval Augmented Generation has the potential to\\nfacilitate rapid prototyping of movie scripts by ingesting\\nthe data in a vector database and retrieving scripts\\nwhich would be most similar with the input prompt.\\nThese retrieved scripts would act as part of context\\nfor the Large Language Models and prevent them from\\nhallucinating. This approach enhances the potential of\\nthese LLMs to generate context-specific output which is\\nessential when taking into consideration the fine details\\nmentioned in the input prompt. Fine-tuning LLMs is\\nanother approach which helps downstream the LLMs to\\nlearn how to generate movie scripts. With the inclusion\\nof visualizing the script elements, one can conveniently\\nturn their ideas into a script and scenes. Combining the\\ncapabilities of Stable Diffusion with the LLMs, script\\ngeneration can be extended to scene generation. On\\ntraining multiple models on the dataset of movie scripts,\\nGemini-Pro (for RAG) was very effective with a cosine\\nsimilarity of 0.5713 whereas GPT-2 and Bloom (for fine-\\ntuning) showed a cosine similarity of 0.5011 and 0.5058\\nrespectively between the input prompt and the generated\\nscript, and perplexity of 1.7443 for GPT-2 and 1.6892\\nfor Bloom showing that GPT-2 is able to generate scripts\\nwhich are coherent and relevant to the input prompt and\\nhas thus understood the language structure and patterns\\nwell. A CLIP score of 0.3061 was achieved with using\\nCompVis for generation of movie scenes.\\nIndex Terms—NLP, Deep Learning, RAG, LLMs, Sta-\\nble Diffusion\\nI. I NTRODUCTION\\nA. Background\\nScriptwriting is an important part of the entertain-\\nment industry. Scripts contain dialogues, scene settings\\nand character development which makes it a time\\nconsuming and arduous process, particularly when\\ntailored content is needed for movies, television shows,\\ngaming and marketing.\\nIt is very important to create a vision of the scene to\\nallow the director to create a scene which engages and\\nappeals to the audience. Along with the increased de-\\nmand for personalized content, automating the process\\nof scriptwriting using Natural Language Processing\\nhas emerged as a promising solution to the problem\\nof generating scalable, high quality scripts along with\\nthe growing demand in the industry.\\nLeveraging Deep Learning models to create com-\\nplex and coherent scripts by retaining context dur-\\ning the generation of the entire script, allowing the\\nscriptwriter to focus on improving the idea itself,\\nsignificantly reduces the time and effort required for\\nscriptwriting.\\nB. Domain\\nNatural Language Generation is a subfield of Natu-\\nral Language Processing. It has proven to be a reliable\\ntool in generating complex and coherent text while\\nretaining contextual nuances throughout the generation\\nprocess.\\nLeveraging deep learning techniques allowing ma-\\nchines to learn the sequential patterns in text, NLG\\nsystems are able to use pre-existing datasets of movie\\nscripts and dialogues to create an understanding of\\nhuman-like dialogue structures. This understanding\\ncan be further used for the purpose of personalized\\nscript generation.\\nExisting tools primarily focus on general text gen-\\neration but have research gaps for developing systems'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 0}, page_content='can be further used for the purpose of personalized\\nscript generation.\\nExisting tools primarily focus on general text gen-\\neration but have research gaps for developing systems\\nthat allow personalization of scripts which include\\ncustomizable elements such as character description,\\nbehavior, and other specifications defined in the input\\nprompt about the scene and setting.\\nC. Motivation\\nStorytelling remains a crucial method of communi-\\ncation in films, gaming and marketing, thereby stream-\\nlining the process of script generation can enable\\nrapid prototyping and customization of inputs such as\\ntime of the day, number of characters and character\\ndescriptions can pioneer significant improvements in\\nplot complexity and quality by allowing writers to\\nfocus more on the plot itself rather than the process of'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 1}, page_content='writing it. This would reduce the barriers for anyone\\nwho wishes to turn their ideas into scripts, promoting\\ncreative freedom and enabling the refinement of ideas\\nrather than focusing on the mechanics of the scripts.\\nD. Contribution\\nAutomation of script generation has the potential\\nof transforming how humans develop and consume\\ncontent globally. As more and more people will have\\nthe convenience of creating high quality scripts out\\nof their imagination and ideas, simply by providing\\nprompts and necessary context to the LLMs or RAG\\nsystems fine-tuned on the movie script’s dataset, fa-\\ncilitating the generation of scripts and make changes\\naccording to preferences without worrying about the\\nback-end complexities of text generation.\\nII. L ITERATURE SURVEY\\nCelikyilmaz A et al. conducted a survey of evalua-\\ntion metrics for text generation including both human\\nand automated methods. Using word embeddings like\\nWord2Vec, Seq2Seq models like GRU and LSTM,\\nBERT and GPT, they highlighted the strengths of\\nhybrid metrics on datasets like WMT, Twitter Dialogue\\nand Daily Mail. Though effective, the study notes the\\nchallenges faced in human evaluation methods and\\nhence the need for standardized methods [1].\\nVasvani et al. introduced the Transformer architec-\\nture which is a fully attention-based approach which\\nenhances text generation tasks. Positional word em-\\nbeddings, shared weight matrix embeddings were used\\nalong with models like GCN, DeepWalk and Planetoid,\\non WMT-2014 dataset. The models though very pow-\\nerful, face a lot of challenges in terms of scalability\\ndue to the quadratic attention complexity [2].\\nLi et al. proposed the Diffusion-LM approach,\\nwhich was effective for controllable text generation\\nwithout the need for retraining. It has been tested\\non E2E and ROCStories datasets, using clustering\\nof learned embeddings, demonstrating strong BLEU-4\\nand ROUGE-L scores. Despite its effectiveness, there\\nare potential issues with high-dimensional tuning and\\nstability of the model in various scenarios [3].\\nFan et al. used over 300 thousand narratives to\\nexplore hierarchical story generation. They used pre-\\ntrained embeddings like FastText with fusion of Con-\\nvolutional, Seq2Seq and self-attention models. Due\\nto the complex architecture, they faced challenges\\nin tuning, but the low perplexity scores show good\\ncoherence in the test dataset [4].\\nBrown et al. showcased the flexibility and adaptabil-\\nity of GPT-3 models in tasks like sentiment analysis\\nand cloze tests. Although GPT-3 models struggled with\\nfine-tuning demands and interpretability issues, they\\nprovided high precision on datasets such as Common-\\nCrawl and WebText2 [5].\\nTambwekar et al. used the encoder-decoder archi-\\ntecture to introduce a reward-shaping technique to\\ngenerate plot-directed stories. They used WordNet and\\nVerbNet Sysnets embeddings and DRL models and\\nachieved low perplexity scores, but the comparisons\\nwere made on the existing corpus and not the unseen\\nplots [6].\\nMangal et al. used the Game of Thrones data to eval-\\nuate different RNN architectures like GRU, LSTM and\\nBidirectional RNN. LSTM and GRU models though\\ngenerated coherent texts, they failed to capture the\\nintricate character descriptions and interactions, indi-\\ncating room for improvement in complex sequences\\n[7].\\nYao et al. introduce the Plan-and-Write framework,\\nintegrating static and dynamic schema for coherence in\\nstory. Using GloVe embeddings and Seq2Seq models\\non ROCStories dataset, the models performed well but\\nfailed to handle complex narrative relationships and\\nevents [8].\\nZhu et al. develop a narrative-guided script genera-\\ntion model by using the Word2Vec embedding with\\nDL2R and MVLSTM models on the GraphMovie\\ndataset. The approach adheres strictly to the structured\\ndesign leading to limitation of creative storytelling [9].\\nBhat et al. apply hierarchical encoders to screen-\\nplay modelling, to classification of genre and mood\\nclassification using Bag-of-Embeddings and GRU on'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 1}, page_content='Bhat et al. apply hierarchical encoders to screen-\\nplay modelling, to classification of genre and mood\\nclassification using Bag-of-Embeddings and GRU on\\nthe ScriptBase-J corpus [10].\\nFang et al. used the WritingPrompts and WikiPlots\\ndatasets and proposed a fine-grained, controllable text\\ngeneration model that builds on cascaded events. They\\nused GloVe, TextRank, RAKE and BERT embed-\\ndings with models like FUSION, FIST and PSA and\\nachieved strong BLEU and ROUGE scores [11].\\nSakaguchi et al. developed the proScript framework\\nto solve the problem of producing high-quality par-\\ntially ordered scripts for everyday scenarios. The study\\nestablished two complementary tasks: script edge pre-\\ndiction and script generation, using a crowd sourced\\ndataset of 6,414 scripts, which was significantly larger\\nthan previous datasets. With the use of pre-trained\\nlanguage models such as T5, the models showed no-\\ntable gains in event generation and script organization,\\nattaining an F1 score of up to 75.71 in edge prediction.\\nDespite the progress, issues like generalization outside\\nof the domain and attaining human-level performance\\npoint to areas that require further study [12].\\nDayo et al. analyzed how AI has transformed con-\\ntemporary scriptwriting, emphasizing how it affects\\nthe cinema business and story development. They\\ncarried out a thorough examination of AI tools such\\nas AI authoring and Natural Language Processing\\n(NLP), showcasing how they might improve interactive\\nstorytelling, character development, and narrative cre-\\nation. Case studies like ”Sunspring” (2016) and ”Zone\\nOut” (2022) showed how human creativity and AI-\\ngenerated content may work together. Despite these\\ndevelopments, there are still issues to be resolved, such\\nas moral dilemmas, how to balance human and AI co-'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 2}, page_content='operation, and the requirement to preserve narrative’s\\nemotional nuance and cultural authenticity [13].\\nIII. M ETHODOLOGY\\nA. Model Architecture\\nThe Transformer architecture [2] is composed of\\ntwo parts: Encoder and the Decoder. The Encoder is\\nresponsible for processing the input sequences and\\ngenerating useful representations of the input (em-\\nbeddings). The Decoder is responsible for using the\\nencoder’s outputs and target sequences to generate\\npredictions.\\nTransformers address the issue of Seq2Seq models\\nthrough Attention Mechanism. In Seq2Seq models, the\\ndecoder uses the ”summarized” version of the input\\nsequence done by the encoder, which raises the issue\\nof lack of context. The Decoder cannot focus on the\\nimportant parts of the input which leads to sub-optimal\\noutputs.\\nThrough Attention Mechanism, transformers gain\\nthe ability to produce context-specific outputs by de-\\ntermining the relevance of each of the tokens in the\\nsequence with respect to others and focusing on certain\\naspects of the input which would help generate optimal\\noutputs. The type of attention mechanisms used in\\nTransformer architecture are Multi-Head and Masked\\nMulti-Head attention mechanisms.\\nFig. 1: Transformer Architecture\\nAttention Mechanism computes the attention scores\\nfor each token to evaluate the relevance of the token\\nwith respect to the output sequence to be generated.\\nTo compute the attention score, three vectors are used:\\n1) Query Matrix (Q): It represents the part of\\ninput sequence the model is focusing on.\\n2) Key Matrix (K): It represents the information\\ncarried by each input token.\\n3) Value Matrix (V): It represents the correspond-\\ning output tokens for which the input tokens are\\nbeing evaluated.\\nEach of the input elements (word embeddings for\\neach token) is projected onto these vectors via some\\nlearned weight matrices. Mathematically, the input\\nrepresentations are defined as:\\nQ = XWQ (1)\\nK = XWK (2)\\nV = XWV (3)\\nwhere X is Input matrix, and WQ, WK, WV are the\\nlearned weight matrices.\\nThe attention score then calculated as a dot product\\nof each element of the Query and Key matrices (q and\\nk respectively) and is scaled using a scaling factor -\\nsquare root of the key dimension dk to prevent large\\nvalues leading to unstable gradients.\\nScore(q, k) = q · k√dk\\n(4)\\nThe raw scores are passed through a softmax func-\\ntion which normalizes these scores to get a probability\\ndistribution, with each probability representing the\\nimportance of the input element.\\nαij = softmax\\n\\x12qi · kj√dk\\n\\x13\\n(5)\\nFinally, the weighted sum of each of the value\\nvectors (v) is computed with the attention scores being\\nthe weights.\\nAttention(Q, K, V) =\\nnX\\nj=1\\nαijvj (6)\\nThe above calculations are for a single-head at-\\ntention mechanism, since transformers use multi-head\\nattention mechanism, multiple single-head attention\\nmechanisms run parallelly and their individual results\\nare concatenated. Along with the concatenated heads\\nand a learned weight matrix WO, the Multi-Head\\nattention score is calculated.\\nMulti-Head(Q, K, V) =Concat(head1, . . . ,headh)WO\\n(7)\\nThe attention mechanism thus assists the transform-\\ners architecture to focus on the important parts of the\\ninput and generate contextually relevant outputs.\\nB. RAG Architecture\\nRAG - Retrieval Augmented Generation, is a frame-\\nwork used to help LLMs provide a better response\\nby providing relevant information from an external\\ndatabase (usually a vector database) which reduces\\nthe chance that the LLM will lead to inaccurate or\\nincomplete responses.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 3}, page_content='Fig. 2: RAG Pipeline\\nLLMs have a tendency of hallucinating when it\\ncomes to generating responses and have a limitation\\nof knowledge, hence RAG helps mitigate these limi-\\ntations of LLMs and provide better responses.\\nThe scripts used for training are converted into\\nchunks using the Recursive Character Text splitter\\nwhich recursively identifies and segments scripts en-\\nsuring that each chunk maintains its coherent context\\nand structure,thereby focusing on specific character\\nscenes and transitions.\\nThese chunks are then converted into embeddings\\nusing sentence transformer in order to capture the\\nsemantic relationships within the chunks and between\\nthe chunks. The chunk embeddings are then stored in a\\nFAISS vector database, along with their metadata like\\nindex of the chunk and name of the movie that chunk\\nis from.\\nThis vector database is used for performing semantic\\nsimilarity search based on a query to retrieve top ’k’\\nchunks which are most similar to the query which will\\nbe used as contextual information for the LLMs, so as\\nto generate contextually relevant scripts (k value used\\nfor the proposed methodology is 5).\\nThe above semantic similarity search is further\\noptimized using IVF-Flat (Inverted File with Flat\\nQuantization), which segregates all the chunks into\\nclusters, this would help the RAG Framework to focus\\non only limited number of chunks for fetching the\\nsemantically most similar chunks rather than searching\\nin the entire database. The clustering is done using\\nthe K-Means Clustering, where in the data points\\nbeing close to each other are considered as ”similar”\\nand are therefore grouped in one cluster. Doing so\\nfor the entire vector database would help divide all\\nthe chunk embeddings into clusters. IVF-Flat thus\\nsignificantly improves upon the time complexity of\\nsearching especially when dealing with a dataset of\\nlarge number of movie scripts.\\nFor a given Query Embedding, the algorithm calcu-\\nlates the L2 Norm (Euclidean Distance) between the\\nQuery Embedding and the Centroid of each region\\n(cluster), in order to narrow-down the search and\\nperform semantic similarity search within that cluster\\nfor the top ’k’ chunks.\\nThe top ’k’ chunks are evaluated using cosine sim-\\nilarity (to measure degree of similarity between two\\nvectors or embeddings) and Euclidean Distance, this\\nway the retrieved chunks are ranked based on how high\\nis the cosine similarity and how low the Euclidean\\nDistance between the Query Embedding and chunk\\nembeddings (with priority given to cosine similarity).\\nThese retrieved chunks (along with their metadata)\\nwill be provided to the LLM to generate an accurate\\nscript which adheres to the input query while prevent-\\ning the LLM from hallucinating. The LLM used for\\nRAG implementation is Gemini-Pro.\\n1) Gemini-Pro\\nGemini-Pro developed by Google DeepMind, is\\na state-of-the-art LLM with enhanced capabilities\\nfor Natural Language Generation and Understanding\\ntasks. It excels in complex reasoning thereby gener-\\nating nuanced and coherent movie scripts tailored to\\nthe specifications. With the inclusion of RAG, Gemini-\\nPro’s potential hallucination is successfully mitigated.\\nGemini-Pro has the ability to use either decoder-\\nonly or encoder-decoder architecture, based on the\\nstructure and type of task, making it a versatile LLM.\\nC. Fine-Tuning LLMs\\nFine-tuning Large Language Models involves using\\na pre-trained model and down-streaming it to a specific\\ntask like script generation by training it on the task-\\nspecific dataset. This process modifies the weights of\\nthe pre-trained model to align the predictions accord-\\ning to the specific task.\\nUsing the embeddings of these pre-trained mod-\\nels ensures that the semantic relationships within the\\nmovie scripts are captured, so as to generate a proper\\ndialogue flow and adhere to specifications.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 4}, page_content='Fig. 3: Fine-Tuning LLMs Pipeline\\n1) GPT-2\\nGPT-2 (Generative Pre-trained Transformer 2) [14]\\nis a state-of-the-art language model based on the trans-\\nformers architecture developed by OpenAI. It consists\\nof multiple self attention mechanisms and feed forward\\nneural networks to generate contextually relevant and\\ncoherent text. GPT-2 is a pre-trained model used for\\na variety of NLP tasks and thus fine tuning it on a\\ndownstream task like text generation would achieve\\ngreat results. The GPT-2 model used in this paper has\\napproximately 124 million parameters (GPT-2 Small).\\nGPT-2 uses its own tokenizer, as other basic tok-\\nenizers are unable to capture the contextual meaning\\nbehind the dialogues and scene settings in a script.\\nIt generates tokens based on the contextual under-\\nstanding of the sequences, thus capturing the relation\\nbetween different sequences.\\nIt tokenizes words into individual characters and\\nmakes character pairs based on consecutive characters.\\nThen it combines two characters into the same token\\nbased on the frequency of the repeating character pairs.\\nThis process is repeated until a predefined vocabulary\\nsize is reached, efficiently and deterministically form-\\ning a set of subword units which represent the text\\ncorpus.\\nThe GPT-2 tokenizer has a limitation of only allow-\\ning a maximum length of 1024 tokens to be encoded.\\nOn average the GPT-2 tokenizer makes approximately\\n4 tokens for every 3 words, therefore the scripts get\\nchunked into 700 word chunks and then tokenized.\\nThe model is then trained on these individual chunks,\\nretaining all information from the entire script.\\nOnce the tokenization is complete, it is ready to be\\nfit into the model for fine-tuning. This approach allows\\nthe model to fine-tune on scripts regardless of their size\\nand helps in retaining context as it trains the model on\\nthe largest possible chunks, learning patterns between\\nwords in a larger sequence.\\n2) Bloom\\nBloom (BigScience Large Open-science Open-\\naccess Multilingual) [15] is another state-of-the-art\\nlanguage model based on the transformers architecture.\\nSimilar to GPT-2, it also consists of multiple self at-\\ntention mechanisms and feed forward neural networks\\nto generate contextually relevant and coherent text.\\nBloom is a pre-trained model which is trained on mul-\\ntilingual data. It has 560 million parameters (Bloom\\n560m) therefore can still prove to have significantly\\npositive results for generating text in a single language.\\nBloom also uses its own tokenizer which tokenizes\\ntext based on a probabilistic approach which is op-\\ntimized for diverse scripts and has a more flexible\\ntokenization approach and allows better capturing of\\ncontext and relationships of words in diverse datasets.\\nIt uses SentencePiece which uses a uni-gram language\\nmodel to build its vocabulary.\\nSentencePiece iteratively learns tokens that are most\\nlikely to occur in the training data. It merges the most\\nfrequently occurring token pairs into larger tokens and\\nprobabilistically estimates the likelihood of merging\\npairs based on observed frequency distributions of\\nmerged tokens. This forms a set of subword units\\nwhich represent the text corpus.\\nThe Bloom tokenizer also has a maximum length of\\n2048 tokens to be encoded. Chunking is also used here\\nto chunk a maximum of 512 tokens per chunk. This\\nallowed for a more efficient tokenization process due\\nto the parallelization of the task using a GPU. After\\nthe tokenization, the tokens are ready to fit into the\\nBloom model for fine-tuning.\\nSince LLM is only using a small percentage of the\\ntotal parameters, there is loss of accuracy in the script\\ngeneration, the key is to find the trade-off between\\nthe loss of accuracy and reducing computational cost\\nwhich would make this approach scalable and efficient\\nfor large and complex datasets.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 5}, page_content='D. Text-Image Generation\\nText-Image generation would allow users to visual-\\nize the scripts through scenes, and hence ideate upon\\nthe movie script generation in a robust manner. Text-\\nImage generation begins with tokenizing and creating\\nan embedding of the query to capture the semantic\\nmeaning and context of the query.\\nFig. 4: Working of Stable Diffusion for Text-Image\\nGeneration\\nTokenization creates tokens which are smaller com-\\nponents of the input query and the embedding is a\\nnumerical vector which represents the tokens. Stable\\nDiffusion [16], one of the best algorithms for image\\ngeneration, begins with random noise and progres-\\nsively learns to refine the noisy image by de-noising\\nit and eventually generating a clear image relevant\\nto the query. It operates within a latent space - a\\ncompressed space in which image generation can be\\ndone efficiently.\\nStable Diffusion uses a process called de-noising\\ndiffusion which refines the noisy latent representation\\nin a step-by-step process to create a meaningful image.\\nThe model aligns the patterns of the image with that\\nof the embedding to form an image which matches the\\nsemantical meaning of the given query. Once the image\\nde-noising is performed, a decoder model is used to\\nupsample this image and transform it into a visible\\nimage.\\nThe algorithm makes use of cross-attention layers\\nwhich allows the model to focus on different parts\\nof the query embedding at various stages of the de-\\nnoising process. This helps in refining the image in\\norder to maintain the coherence and semantic integrity\\nof the input text during the generation of the image.\\nThe Stable Diffusion model used for this approach\\nis CompVis.\\nIV. E VALUATION\\n1) Cosine Similarity:\\nCosine Similarity is a popular metric for assess-\\ning how semantically similar a generated script\\nis to its input prompt. In a multidimensional\\nspace, it is calculated as the cosine of the angle\\nformed by two vectors. The input prompt and the\\nresulting script’s text are represented by these\\nvectors, respectively.\\nThe range of a cosine similarity value is -1 to\\n1. A value of 1 denotes complete alignment, in\\nwhich the output script’s semantic meaning is\\nexactly the same as that of the input prompt.\\nBeing orthogonal, a value of 0 indicates no\\nsemantic resemblance. A value of -1, on the\\nother hand, indicates total dissimilarity, with the\\nvectors pointing in opposing directions.\\nThe model’s ability to preserve semantic co-\\nherence between the input and the generated\\noutput improves with increasing cosine similar-\\nity. When guaranteeing semantic consistency in\\ntext creation is a primary goal, this metric is\\nessential.\\nCosine Similarity(A, B) = A · B\\n∥A∥∥B∥ (8)\\nwhere, A and B are vectors.\\n2) Perplexity:\\nPerplexity is another crucial assessment metric ,\\nwhich is mainly applied to text generation tasks.\\nIt evaluates a probabilistic model’s ability to\\nanticipate a word sequence. The entropy of the\\nanticipated probability is used to calculate the\\nperplexity value, which goes from 1 to infinity.\\nThe model is more certain of its predictions for\\nthe subsequent token in the sequence when the\\nperplexity score is lower. On the other hand,\\ngreater uncertainty is indicated by a larger per-\\nplexity, which exposes flaws in the probability\\ndistribution of the model. A diagnostic method\\nfor assessing a language model’s dependability\\nand fluency is perplexity.\\nLow perplexity is a crucial parameter for as-\\nsessing performance in natural language genera-\\ntion tasks since models with low perplexity are\\ngenerally better able to generate coherent and\\ncontextually relevant sequences. Mathematically,\\nPerplexity (PPL) is calculated as:\\nPPL = exp\\n \\n− 1\\nN\\nNX\\ni=1\\nlog P(wi | w1, . . . , wi−1)\\n!\\n(9)\\nwhere, P(wi | w1, . . . , wi−1) is the conditional\\nprobability of the i-th word given all the previous\\nwords w1, . . . , wi−1 in the sequence and N is\\nthe total number of words in the test sequence.\\n3) CLIP Score:\\nIt is a metric used to assess how well created'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 5}, page_content='words w1, . . . , wi−1 in the sequence and N is\\nthe total number of words in the test sequence.\\n3) CLIP Score:\\nIt is a metric used to assess how well created\\nimages match the written prompts that go with\\nthem. CLIP generates vector representations for\\nthe created image and the text prompt using\\na shared embedding space. Cosine similarity is'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 6}, page_content='then used to calculate how similar these embed-\\ndings are to one another.\\nWhile a lower score signifies poor alignment,\\na higher CLIP Score shows that the resulting\\nimage closely reflects the text prompt’s semantic\\nintent. In multi-modal applications where pre-\\nserving the prompt’s integrity in the visual rep-\\nresentation is crucial, such as picture synthesis\\nfrom textual descriptions, this measure is very\\nuseful.\\nThe CLIP Score guarantees that produced out-\\nputs satisfy the required degree of semantic\\naccuracy by offering a measurable indicator of\\ntextual and visual coherence.\\nCLIP(T, I) = ET · EI\\n∥ET ∥∥EI∥ (10)\\nwhere, ET and EI are the embeddings of the\\ntext and image, respectively and ∥ET ∥ and ∥EI∥\\nrepresent the norms (magnitudes) of these em-\\nbedding vectors.\\nV. R ESULTS\\nTABLE I: Model evaluation metrics and their average\\nscores\\nModel Metric Score (Average)\\nGPT-2 Cosine Similarity 0.5011\\nGPT-2 Perplexity 1.7443\\nBloom Cosine Similarity 0.5058\\nBloom Perplexity 1.6892\\nGemini-Pro Cosine Similarity 0.5713\\nCompVis CLIP Score 0.3061\\n1) GPT-2 (Cosine Similarity):\\nFig.5 shows a density plot of cosine similarity\\nusing the fine-tuned GPT-2 model. The values\\ngo as high as 0.8 and as low as 0.2. The graph\\nsuggests that the generated script at times has\\nvery high contextual similarity between the input\\nquery and the generated script and at times very\\nlow.\\nFig. 5: Density Plot for Cosine Similarity (fine-tuned\\nGPT-2)\\nThe lower values can suggest poor generation\\nleading to lesser context being included in the\\ngenerated script or can be due to high creativity\\nwhich leads to a variety of information along\\nwith the details outlined in the prompt being\\nmentioned in the script.\\nThe higher values suggest that the script per-\\nfectly encompasses the required details from the\\nprompt but can also suggest that the generation\\nhad lesser creativity and did not add many details\\nwhich were not mentioned. This can suggest\\nthat a great input prompt with a lot of details\\nwas mentioned or the generation was with poor\\ncreativity.\\nMajority of the cosine similarity values lying\\nnear the average of 0.5011 suggests that the\\napproach was able to generate a script which\\ncontained the details from the prompt and was\\nalso able to include creative aspects in the script.\\n2) GPT-2 (Perplexity):\\nFig.6 shows a density plot of the perplexity using\\nfine-tuned GPT-2. The perplexity had an average\\nof 1.7443. The perplexity value varied on the\\nlower side and upper side of this average value.\\nThe lower values suggest that the model was\\nless surprised in the token generation of these\\nscripts leading to a well structured script where\\nfollowup words were expected more often than\\nnot.\\nFig. 6: Density Plot for Perplexity (GPT-2)\\nThe higher values can signify a script which\\nmade the model more surprised during the token\\ngeneration. This suggests that the grammatical\\nand semantic clarity of the sentences was jeopar-\\ndized and the sentences more often than not, did\\nnot make sense grammatically and contextually.\\nThe scripts with perplexity near the average\\nvalue can suggest that they made sense gram-\\nmatically and contextually to some degree but\\ncould have either left something to be desired\\nin the generation or the creative aspect of the\\ngeneration left the model more surprised at the\\ntoken generation, attributing to its slightly higher\\nperplexity score. This could suggest that the\\nideal value of perplexity for script generation\\nwhich includes good creativity could be that with\\nvalues near the average perplexity of 1.7443.\\n3) Bloom (Cosine Similarity):'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 7}, page_content='Fig.7 shows a density plot of cosine similar-\\nity using the fine-tuned Bloom model, which\\ndiffers from the fine-tuned GPT-2 model in its\\ndistribution and average values. While the cosine\\nsimilarity for GPT-2 averaged at 0.5011, the\\nBloom model’s average was 0.5058.\\nFig. 7: Density Plot for Cosine Similarity (fine-tuned\\nBloom)\\nBoth models showed a range of values from\\n0.2 to 0.8, indicating varying levels of con-\\ntextual similarity between the input query and\\nthe generated script. However, the Bloom model\\nappears to strike a slightly different balance, with\\nits clustering around the average suggesting a\\nconsistent ability to include prompt details while\\nintroducing creative aspects.\\nIn comparison, the GPT-2 model’s lower and\\nhigher values might more distinctly reflect out-\\nputs that either deviate significantly from the\\nprompt or align too closely, potentially at the\\nexpense of creativity. The differences in average\\nvalues and distributions suggest that Bloom may\\nprovide a more balanced generation, while GPT-\\n2 may exhibit greater variance in its outputs\\ndepending on the nature of the prompt.\\nAccording to the variations in average values\\nand distributions, Bloom might produce a more\\nbalanced generation, but GPT-2 might produce\\noutputs that vary more depending on the prompt.\\n4) Bloom (Perplexity):\\nFig.6 compares the density plots of perplexity\\nfor fine-tuned GPT-2 and Bloom models, demon-\\nstrating variations in their text generation. GPT-2\\nrecorded an average perplexity of 1.7443, while\\nBloom achieved a lesser average of 1.6892.\\nFig. 8: Density Plot for Perplexity (fine-tuned Bloom)\\nHigher perplexity values for GPT-2 indicate dif-\\nficulties in preserving grammatical or semantic\\nclarity, which frequently results in outputs that\\nlack contextual sense. Lower perplexity val-\\nues, on the other hand, indicate well-structured\\nscripts with predictable token generation. Al-\\nthough some may have had space for growth in\\ngeneration quality, scripts with ambiguity close\\nto the average demonstrated a balance between\\ncoherence and inventiveness.\\nBloom’s tighter grouping and lower confusion,\\non the other hand, point to a more reliable\\ncapacity for producing language that is both con-\\ntextually relevant and coherent. Its values close\\nto the mean show a steady equilibrium between\\nfollowing the prompt and adding originality.\\nBloom’s smaller range of perplexity scores sug-\\ngests a lower variability than GPT-2, which\\nmakes it more dependable for applications de-\\nmanding accuracy. However, applications that\\nvalue diversity and flexibility in script produc-\\ntion could find GPT-2 more appropriate due to\\nits wider range\\n5) Gemini-Pro (Cosine Similarity):\\nFig.9 shows the density plot shows the distri-\\nbution of cosine similarity values for outputs\\nproduced by the Gemini-Pro model using RAG,\\nwith an average cosine similarity of 0.5713.The\\nrange is from 0.2 to 0.8.\\nThis variability shows how well the model per-\\nforms in striking a balance between contex-\\ntual alignment of generated scripts and creative\\nflexibility; lower cosine similarity values, closer\\nto 0.2, indicate instances where the generated\\nscripts either lack sufficient context from the in-\\nput prompt or exhibit high creativity by deviating\\nfrom the input to include diverse information;\\nhigher values, approaching 0.8, indicate outputs\\nthat closely align with the prompt, effectively\\ncapturing its details; however, such outputs may\\nalso suggest limited creativity, as the generation\\nmay strictly follow the prompt without adding\\nsubtle or unique information.\\nFig. 9: Density Plot for Cosine Similarity (Gemini-\\nPro)\\nThe concentration of cosine similarity values\\naround the average of 0.5713 shows that Gemini-'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 8}, page_content='Pro successfully strikes a balance between cre-\\nativity and relevance, generating scripts that use\\nRAG to add meaningful and creative elabora-\\ntion to the content while incorporating relevant\\ninformation from the prompt.This performance\\nshows how well Gemini-Pro combines creative\\nflexibility with rapid alignment, outperforming\\nother models in the process.\\n6) CompVis (CLIP Score):\\nFig.10 shows a density plot for the CLIP score\\nof the image generated from the input prompt.\\nThe average CLIP score was 0.3061.\\nThis shows a weaker alignment of the image\\nwith the features mentioned in the prompt or\\ncould also suggest that the features in the image\\nare loosely related to the details in the prompt.\\nThis could be due to the model under-performing\\nor the input prompts not being detailed enough\\nto generate distinct features in the image.\\nFig. 10: Density Plot for CLIP Score (CompVis)\\nAside from lack of details in the prompt, abstract\\nor complex details could also lead to a lower\\nCLIP score since the model might be unable to\\nmake a strong connection between these con-\\ncepts and features in the image.\\n7) Reason why Gemini-Pro cannot be evalu-\\nated using Perplexity: To calculate perplexity,\\na probabilistic token-based prediction approach\\nis required but since Gemini-Pro is being inte-\\ngrated with the RAG Pipeline, it is not using\\nany probability distribution to predict the next\\ntokens, it rather combines the retrieval mecha-\\nnisms of RAG with generation.\\nSince RAG models do not use a probabilistic\\napproach to predict the next tokens, Gemini-Pro\\nthus does not produce a probability distribution\\nand perplexity cannot be used as an evaluation\\nmetric for Gemini-Pro.\\nVI. F UTURE SCOPE\\nDue to hardware constraints, the LLMs used in the\\nproposed methodology have limited capabilities. Al-\\nthough LLMs provided accurate responses to the query\\nand performed well on the evaluation metrics, better\\nmodels with a greater number of parameters can be\\nused to further refine the movie scripts to adhere to the\\nspecifications in a better way. Better stable diffusion\\nmodels could also be used to generate accurate text\\ninside an image, or generate higher quality images for\\nbetter visualization of the generated movie script.\\nThe models could also be optimized to give sug-\\ngestions for what kind of lighting or scene setting is\\nneeded to direct such a movie script in an apt manner\\nby storing more metadata about the script in the vector\\ndatabase.\\nVII. C ONCLUSION\\nThe proposed methodologies demonstrate the poten-\\ntial of advanced NLP techniques like RAG and Fine-\\nTuning LLMs, and Image Generation algorithms like\\nStable Diffusion in order to automate script and scene\\ngeneration in the cinema industry.\\nThe lower perplexity and high cosine similarity\\nvalues indicate a high level of coherence and fluency\\nin generated dialogues, which is important for main-\\ntaining human-like script writing techniques. CLIP\\nScore indicates that despite the room for improvement,\\nStable Diffusion can help visualize the input query in\\nthe form of a movie scene with a moderate level of\\nalignment.\\nThis approach reduces the barriers in creative pro-\\ncesses, allowing a better overall script generation.\\nACKNOWLEDGMENT\\nWe would like to extend our sincere gratitude to\\nSymbiosis Institute of Technology, Symbiosis Interna-\\ntional (Deemed University), Pune, India, for providing\\nthe invaluable support and platform necessary to con-\\nduct our research work. We are thankful for providing\\nthe nurturing environment where we had the privilege\\nto learn and explore.\\nREFERENCES\\n[1] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Eval-\\nuation of text generation: A survey. arXiv preprint\\narXiv:2006.14799, 2020.\\n[2] A Vaswani. Attention is all you need. Advances in Neural\\nInformation Processing Systems , 2017.\\n[3] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,\\nand Tatsunori B Hashimoto. Diffusion-lm improves con-\\ntrollable text generation. Advances in Neural Information'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 8}, page_content='[3] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,\\nand Tatsunori B Hashimoto. Diffusion-lm improves con-\\ntrollable text generation. Advances in Neural Information\\nProcessing Systems, 35:4328–4343, 2022.\\n[4] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical\\nneural story generation. arXiv preprint arXiv:1805.04833 ,\\n2018.\\n[5] Tom B Brown. Language models are few-shot learners. arXiv\\npreprint arXiv:2005.14165, 2020.\\n[6] Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J Martin,\\nAnimesh Mehta, Brent Harrison, and Mark O Riedl. Control-\\nlable neural story plot generation via reward shaping. arXiv\\npreprint arXiv:1809.10736, 2018.\\n[7] Sanidhya Mangal, Poorva Joshi, and Rahul Modak. Lstm vs.\\ngru vs. bidirectional rnn for script generation. arXiv preprint\\narXiv:1908.04332, 2019.\\n[8] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight,\\nDongyan Zhao, and Rui Yan. Plan-and-write: Towards better\\nautomatic storytelling. In Proceedings of the AAAI Conference\\non Artificial Intelligence , volume 33, pages 7378–7385, 2019.\\n[9] Yutao Zhu, Ruihua Song, Zhicheng Dou, Jian-Yun Nie, and Jin\\nZhou. Scriptwriter: Narrative-guided script generation. arXiv\\npreprint arXiv:2005.10331, 2020.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Ansh Lulla\\\\VS-Code\\\\Langchain_tutorial\\\\summarization\\\\PlotPoint_Research_Paper.pdf', 'page': 9}, page_content='[10] Gayatri Bhat, Avneesh Saluja, Melody Dye, and Jan Flor-\\njanczyk. Hierarchical encoders for modeling and interpreting\\nscreenplays. arXiv preprint arXiv:2004.14532 , 2020.\\n[11] Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong,\\nand Changyou Chen. Outline to story: Fine-grained control-\\nlable story generation from cascaded events. arXiv preprint\\narXiv:2101.00822, 2021.\\n[12] Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras,\\nNiket Tandon, Peter Clark, and Yejin Choi. proscript: Partially\\nordered scripts generation via pre-trained language models.\\narXiv preprint arXiv:2104.08251 , 2021.\\n[13] Fatima Dayo, Ahmed Ali Memon, and Nasrullah Dharejo.\\nScriptwriting in the age of ai: Revolutionizing storytelling with\\nartificial intelligence. Journal of Media & Communication ,\\n4(1):24–38, 2023.\\n[14] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\\nAmodei, Ilya Sutskever, et al. Language models are unsuper-\\nvised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[15] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,\\nSuzana Ili ´c, Daniel Hesslow, Roman Castagn ´e, Alexan-\\ndra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e, et al.\\nBloom: A 176b-parameter open-access multilingual language\\nmodel. 2023.\\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\\nEsser, and Bj ¨orn Ommer. High-resolution image synthesis\\nwith latent diffusion models. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition , pages\\n10684–10695, 2022.\\n[17] Y Zhang. Dialogpt: Large-scale generative pre-training\\nfor conversational response generation. arXiv preprint\\narXiv:1911.00536, 2019.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=r\"C:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\summarization\\PlotPoint_Research_Paper.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\n    Write a concise summary of the document.\\n    text: {text}\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    Write a concise summary of the document.\n",
    "    text: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002D79AE2D730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002D798931DC0>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama3-8b-8192\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\n    Write a concise summary of the document.\\n    text: {text}\\n'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002D79AE2D730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002D798931DC0>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='text')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all the docs and create a summary\n",
    "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=prompt, verbose=True)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh Lulla\\AppData\\Local\\Temp\\ipykernel_8224\\440227983.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = chain.run(docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Write a concise summary of the document.\n",
      "    text: PlotPoint - An AI-Driven Framework for\n",
      "Automated Movie Scriptwriting and Scene\n",
      "Generation\n",
      "Ansh Lulla, Aayush Koul, Rampalli Agni Mithra,\n",
      "Aniket K. Shahade ∗, Mayur Gaikwad, Shruti Patil\n",
      "Department of AIML ,\n",
      "Symbiosis Institute of Technology, Pune Campus , Symbiosis International (Deemed University), Pune, India\n",
      "∗aniket.shahade@sitpune.edu.in\n",
      "Abstract—Script writing has become a labor-intensive\n",
      "process, especially when tailored to specific scene setting,\n",
      "culture or character. Recent advancements in the field\n",
      "of Natural Language Processing and Deep Learning\n",
      "algorithms have made it possible to automate the process\n",
      "of script writing. With a simple prompt, it is possible to\n",
      "generate the entire script for a movie, which would be\n",
      "customizable by the input’s creativity and imagination.\n",
      "Retrieval Augmented Generation has the potential to\n",
      "facilitate rapid prototyping of movie scripts by ingesting\n",
      "the data in a vector database and retrieving scripts\n",
      "which would be most similar with the input prompt.\n",
      "These retrieved scripts would act as part of context\n",
      "for the Large Language Models and prevent them from\n",
      "hallucinating. This approach enhances the potential of\n",
      "these LLMs to generate context-specific output which is\n",
      "essential when taking into consideration the fine details\n",
      "mentioned in the input prompt. Fine-tuning LLMs is\n",
      "another approach which helps downstream the LLMs to\n",
      "learn how to generate movie scripts. With the inclusion\n",
      "of visualizing the script elements, one can conveniently\n",
      "turn their ideas into a script and scenes. Combining the\n",
      "capabilities of Stable Diffusion with the LLMs, script\n",
      "generation can be extended to scene generation. On\n",
      "training multiple models on the dataset of movie scripts,\n",
      "Gemini-Pro (for RAG) was very effective with a cosine\n",
      "similarity of 0.5713 whereas GPT-2 and Bloom (for fine-\n",
      "tuning) showed a cosine similarity of 0.5011 and 0.5058\n",
      "respectively between the input prompt and the generated\n",
      "script, and perplexity of 1.7443 for GPT-2 and 1.6892\n",
      "for Bloom showing that GPT-2 is able to generate scripts\n",
      "which are coherent and relevant to the input prompt and\n",
      "has thus understood the language structure and patterns\n",
      "well. A CLIP score of 0.3061 was achieved with using\n",
      "CompVis for generation of movie scenes.\n",
      "Index Terms—NLP, Deep Learning, RAG, LLMs, Sta-\n",
      "ble Diffusion\n",
      "I. I NTRODUCTION\n",
      "A. Background\n",
      "Scriptwriting is an important part of the entertain-\n",
      "ment industry. Scripts contain dialogues, scene settings\n",
      "and character development which makes it a time\n",
      "consuming and arduous process, particularly when\n",
      "tailored content is needed for movies, television shows,\n",
      "gaming and marketing.\n",
      "It is very important to create a vision of the scene to\n",
      "allow the director to create a scene which engages and\n",
      "appeals to the audience. Along with the increased de-\n",
      "mand for personalized content, automating the process\n",
      "of scriptwriting using Natural Language Processing\n",
      "has emerged as a promising solution to the problem\n",
      "of generating scalable, high quality scripts along with\n",
      "the growing demand in the industry.\n",
      "Leveraging Deep Learning models to create com-\n",
      "plex and coherent scripts by retaining context dur-\n",
      "ing the generation of the entire script, allowing the\n",
      "scriptwriter to focus on improving the idea itself,\n",
      "significantly reduces the time and effort required for\n",
      "scriptwriting.\n",
      "B. Domain\n",
      "Natural Language Generation is a subfield of Natu-\n",
      "ral Language Processing. It has proven to be a reliable\n",
      "tool in generating complex and coherent text while\n",
      "retaining contextual nuances throughout the generation\n",
      "process.\n",
      "Leveraging deep learning techniques allowing ma-\n",
      "chines to learn the sequential patterns in text, NLG\n",
      "systems are able to use pre-existing datasets of movie\n",
      "scripts and dialogues to create an understanding of\n",
      "human-like dialogue structures. This understanding\n",
      "can be further used for the purpose of personalized\n",
      "script generation.\n",
      "Existing tools primarily focus on general text gen-\n",
      "eration but have research gaps for developing systems\n",
      "\n",
      "can be further used for the purpose of personalized\n",
      "script generation.\n",
      "Existing tools primarily focus on general text gen-\n",
      "eration but have research gaps for developing systems\n",
      "that allow personalization of scripts which include\n",
      "customizable elements such as character description,\n",
      "behavior, and other specifications defined in the input\n",
      "prompt about the scene and setting.\n",
      "C. Motivation\n",
      "Storytelling remains a crucial method of communi-\n",
      "cation in films, gaming and marketing, thereby stream-\n",
      "lining the process of script generation can enable\n",
      "rapid prototyping and customization of inputs such as\n",
      "time of the day, number of characters and character\n",
      "descriptions can pioneer significant improvements in\n",
      "plot complexity and quality by allowing writers to\n",
      "focus more on the plot itself rather than the process of\n",
      "\n",
      "writing it. This would reduce the barriers for anyone\n",
      "who wishes to turn their ideas into scripts, promoting\n",
      "creative freedom and enabling the refinement of ideas\n",
      "rather than focusing on the mechanics of the scripts.\n",
      "D. Contribution\n",
      "Automation of script generation has the potential\n",
      "of transforming how humans develop and consume\n",
      "content globally. As more and more people will have\n",
      "the convenience of creating high quality scripts out\n",
      "of their imagination and ideas, simply by providing\n",
      "prompts and necessary context to the LLMs or RAG\n",
      "systems fine-tuned on the movie script’s dataset, fa-\n",
      "cilitating the generation of scripts and make changes\n",
      "according to preferences without worrying about the\n",
      "back-end complexities of text generation.\n",
      "II. L ITERATURE SURVEY\n",
      "Celikyilmaz A et al. conducted a survey of evalua-\n",
      "tion metrics for text generation including both human\n",
      "and automated methods. Using word embeddings like\n",
      "Word2Vec, Seq2Seq models like GRU and LSTM,\n",
      "BERT and GPT, they highlighted the strengths of\n",
      "hybrid metrics on datasets like WMT, Twitter Dialogue\n",
      "and Daily Mail. Though effective, the study notes the\n",
      "challenges faced in human evaluation methods and\n",
      "hence the need for standardized methods [1].\n",
      "Vasvani et al. introduced the Transformer architec-\n",
      "ture which is a fully attention-based approach which\n",
      "enhances text generation tasks. Positional word em-\n",
      "beddings, shared weight matrix embeddings were used\n",
      "along with models like GCN, DeepWalk and Planetoid,\n",
      "on WMT-2014 dataset. The models though very pow-\n",
      "erful, face a lot of challenges in terms of scalability\n",
      "due to the quadratic attention complexity [2].\n",
      "Li et al. proposed the Diffusion-LM approach,\n",
      "which was effective for controllable text generation\n",
      "without the need for retraining. It has been tested\n",
      "on E2E and ROCStories datasets, using clustering\n",
      "of learned embeddings, demonstrating strong BLEU-4\n",
      "and ROUGE-L scores. Despite its effectiveness, there\n",
      "are potential issues with high-dimensional tuning and\n",
      "stability of the model in various scenarios [3].\n",
      "Fan et al. used over 300 thousand narratives to\n",
      "explore hierarchical story generation. They used pre-\n",
      "trained embeddings like FastText with fusion of Con-\n",
      "volutional, Seq2Seq and self-attention models. Due\n",
      "to the complex architecture, they faced challenges\n",
      "in tuning, but the low perplexity scores show good\n",
      "coherence in the test dataset [4].\n",
      "Brown et al. showcased the flexibility and adaptabil-\n",
      "ity of GPT-3 models in tasks like sentiment analysis\n",
      "and cloze tests. Although GPT-3 models struggled with\n",
      "fine-tuning demands and interpretability issues, they\n",
      "provided high precision on datasets such as Common-\n",
      "Crawl and WebText2 [5].\n",
      "Tambwekar et al. used the encoder-decoder archi-\n",
      "tecture to introduce a reward-shaping technique to\n",
      "generate plot-directed stories. They used WordNet and\n",
      "VerbNet Sysnets embeddings and DRL models and\n",
      "achieved low perplexity scores, but the comparisons\n",
      "were made on the existing corpus and not the unseen\n",
      "plots [6].\n",
      "Mangal et al. used the Game of Thrones data to eval-\n",
      "uate different RNN architectures like GRU, LSTM and\n",
      "Bidirectional RNN. LSTM and GRU models though\n",
      "generated coherent texts, they failed to capture the\n",
      "intricate character descriptions and interactions, indi-\n",
      "cating room for improvement in complex sequences\n",
      "[7].\n",
      "Yao et al. introduce the Plan-and-Write framework,\n",
      "integrating static and dynamic schema for coherence in\n",
      "story. Using GloVe embeddings and Seq2Seq models\n",
      "on ROCStories dataset, the models performed well but\n",
      "failed to handle complex narrative relationships and\n",
      "events [8].\n",
      "Zhu et al. develop a narrative-guided script genera-\n",
      "tion model by using the Word2Vec embedding with\n",
      "DL2R and MVLSTM models on the GraphMovie\n",
      "dataset. The approach adheres strictly to the structured\n",
      "design leading to limitation of creative storytelling [9].\n",
      "Bhat et al. apply hierarchical encoders to screen-\n",
      "play modelling, to classification of genre and mood\n",
      "classification using Bag-of-Embeddings and GRU on\n",
      "\n",
      "Bhat et al. apply hierarchical encoders to screen-\n",
      "play modelling, to classification of genre and mood\n",
      "classification using Bag-of-Embeddings and GRU on\n",
      "the ScriptBase-J corpus [10].\n",
      "Fang et al. used the WritingPrompts and WikiPlots\n",
      "datasets and proposed a fine-grained, controllable text\n",
      "generation model that builds on cascaded events. They\n",
      "used GloVe, TextRank, RAKE and BERT embed-\n",
      "dings with models like FUSION, FIST and PSA and\n",
      "achieved strong BLEU and ROUGE scores [11].\n",
      "Sakaguchi et al. developed the proScript framework\n",
      "to solve the problem of producing high-quality par-\n",
      "tially ordered scripts for everyday scenarios. The study\n",
      "established two complementary tasks: script edge pre-\n",
      "diction and script generation, using a crowd sourced\n",
      "dataset of 6,414 scripts, which was significantly larger\n",
      "than previous datasets. With the use of pre-trained\n",
      "language models such as T5, the models showed no-\n",
      "table gains in event generation and script organization,\n",
      "attaining an F1 score of up to 75.71 in edge prediction.\n",
      "Despite the progress, issues like generalization outside\n",
      "of the domain and attaining human-level performance\n",
      "point to areas that require further study [12].\n",
      "Dayo et al. analyzed how AI has transformed con-\n",
      "temporary scriptwriting, emphasizing how it affects\n",
      "the cinema business and story development. They\n",
      "carried out a thorough examination of AI tools such\n",
      "as AI authoring and Natural Language Processing\n",
      "(NLP), showcasing how they might improve interactive\n",
      "storytelling, character development, and narrative cre-\n",
      "ation. Case studies like ”Sunspring” (2016) and ”Zone\n",
      "Out” (2022) showed how human creativity and AI-\n",
      "generated content may work together. Despite these\n",
      "developments, there are still issues to be resolved, such\n",
      "as moral dilemmas, how to balance human and AI co-\n",
      "\n",
      "operation, and the requirement to preserve narrative’s\n",
      "emotional nuance and cultural authenticity [13].\n",
      "III. M ETHODOLOGY\n",
      "A. Model Architecture\n",
      "The Transformer architecture [2] is composed of\n",
      "two parts: Encoder and the Decoder. The Encoder is\n",
      "responsible for processing the input sequences and\n",
      "generating useful representations of the input (em-\n",
      "beddings). The Decoder is responsible for using the\n",
      "encoder’s outputs and target sequences to generate\n",
      "predictions.\n",
      "Transformers address the issue of Seq2Seq models\n",
      "through Attention Mechanism. In Seq2Seq models, the\n",
      "decoder uses the ”summarized” version of the input\n",
      "sequence done by the encoder, which raises the issue\n",
      "of lack of context. The Decoder cannot focus on the\n",
      "important parts of the input which leads to sub-optimal\n",
      "outputs.\n",
      "Through Attention Mechanism, transformers gain\n",
      "the ability to produce context-specific outputs by de-\n",
      "termining the relevance of each of the tokens in the\n",
      "sequence with respect to others and focusing on certain\n",
      "aspects of the input which would help generate optimal\n",
      "outputs. The type of attention mechanisms used in\n",
      "Transformer architecture are Multi-Head and Masked\n",
      "Multi-Head attention mechanisms.\n",
      "Fig. 1: Transformer Architecture\n",
      "Attention Mechanism computes the attention scores\n",
      "for each token to evaluate the relevance of the token\n",
      "with respect to the output sequence to be generated.\n",
      "To compute the attention score, three vectors are used:\n",
      "1) Query Matrix (Q): It represents the part of\n",
      "input sequence the model is focusing on.\n",
      "2) Key Matrix (K): It represents the information\n",
      "carried by each input token.\n",
      "3) Value Matrix (V): It represents the correspond-\n",
      "ing output tokens for which the input tokens are\n",
      "being evaluated.\n",
      "Each of the input elements (word embeddings for\n",
      "each token) is projected onto these vectors via some\n",
      "learned weight matrices. Mathematically, the input\n",
      "representations are defined as:\n",
      "Q = XWQ (1)\n",
      "K = XWK (2)\n",
      "V = XWV (3)\n",
      "where X is Input matrix, and WQ, WK, WV are the\n",
      "learned weight matrices.\n",
      "The attention score then calculated as a dot product\n",
      "of each element of the Query and Key matrices (q and\n",
      "k respectively) and is scaled using a scaling factor -\n",
      "square root of the key dimension dk to prevent large\n",
      "values leading to unstable gradients.\n",
      "Score(q, k) = q · k√dk\n",
      "(4)\n",
      "The raw scores are passed through a softmax func-\n",
      "tion which normalizes these scores to get a probability\n",
      "distribution, with each probability representing the\n",
      "importance of the input element.\n",
      "αij = softmax\n",
      "\u0012qi · kj√dk\n",
      "\u0013\n",
      "(5)\n",
      "Finally, the weighted sum of each of the value\n",
      "vectors (v) is computed with the attention scores being\n",
      "the weights.\n",
      "Attention(Q, K, V) =\n",
      "nX\n",
      "j=1\n",
      "αijvj (6)\n",
      "The above calculations are for a single-head at-\n",
      "tention mechanism, since transformers use multi-head\n",
      "attention mechanism, multiple single-head attention\n",
      "mechanisms run parallelly and their individual results\n",
      "are concatenated. Along with the concatenated heads\n",
      "and a learned weight matrix WO, the Multi-Head\n",
      "attention score is calculated.\n",
      "Multi-Head(Q, K, V) =Concat(head1, . . . ,headh)WO\n",
      "(7)\n",
      "The attention mechanism thus assists the transform-\n",
      "ers architecture to focus on the important parts of the\n",
      "input and generate contextually relevant outputs.\n",
      "B. RAG Architecture\n",
      "RAG - Retrieval Augmented Generation, is a frame-\n",
      "work used to help LLMs provide a better response\n",
      "by providing relevant information from an external\n",
      "database (usually a vector database) which reduces\n",
      "the chance that the LLM will lead to inaccurate or\n",
      "incomplete responses.\n",
      "\n",
      "Fig. 2: RAG Pipeline\n",
      "LLMs have a tendency of hallucinating when it\n",
      "comes to generating responses and have a limitation\n",
      "of knowledge, hence RAG helps mitigate these limi-\n",
      "tations of LLMs and provide better responses.\n",
      "The scripts used for training are converted into\n",
      "chunks using the Recursive Character Text splitter\n",
      "which recursively identifies and segments scripts en-\n",
      "suring that each chunk maintains its coherent context\n",
      "and structure,thereby focusing on specific character\n",
      "scenes and transitions.\n",
      "These chunks are then converted into embeddings\n",
      "using sentence transformer in order to capture the\n",
      "semantic relationships within the chunks and between\n",
      "the chunks. The chunk embeddings are then stored in a\n",
      "FAISS vector database, along with their metadata like\n",
      "index of the chunk and name of the movie that chunk\n",
      "is from.\n",
      "This vector database is used for performing semantic\n",
      "similarity search based on a query to retrieve top ’k’\n",
      "chunks which are most similar to the query which will\n",
      "be used as contextual information for the LLMs, so as\n",
      "to generate contextually relevant scripts (k value used\n",
      "for the proposed methodology is 5).\n",
      "The above semantic similarity search is further\n",
      "optimized using IVF-Flat (Inverted File with Flat\n",
      "Quantization), which segregates all the chunks into\n",
      "clusters, this would help the RAG Framework to focus\n",
      "on only limited number of chunks for fetching the\n",
      "semantically most similar chunks rather than searching\n",
      "in the entire database. The clustering is done using\n",
      "the K-Means Clustering, where in the data points\n",
      "being close to each other are considered as ”similar”\n",
      "and are therefore grouped in one cluster. Doing so\n",
      "for the entire vector database would help divide all\n",
      "the chunk embeddings into clusters. IVF-Flat thus\n",
      "significantly improves upon the time complexity of\n",
      "searching especially when dealing with a dataset of\n",
      "large number of movie scripts.\n",
      "For a given Query Embedding, the algorithm calcu-\n",
      "lates the L2 Norm (Euclidean Distance) between the\n",
      "Query Embedding and the Centroid of each region\n",
      "(cluster), in order to narrow-down the search and\n",
      "perform semantic similarity search within that cluster\n",
      "for the top ’k’ chunks.\n",
      "The top ’k’ chunks are evaluated using cosine sim-\n",
      "ilarity (to measure degree of similarity between two\n",
      "vectors or embeddings) and Euclidean Distance, this\n",
      "way the retrieved chunks are ranked based on how high\n",
      "is the cosine similarity and how low the Euclidean\n",
      "Distance between the Query Embedding and chunk\n",
      "embeddings (with priority given to cosine similarity).\n",
      "These retrieved chunks (along with their metadata)\n",
      "will be provided to the LLM to generate an accurate\n",
      "script which adheres to the input query while prevent-\n",
      "ing the LLM from hallucinating. The LLM used for\n",
      "RAG implementation is Gemini-Pro.\n",
      "1) Gemini-Pro\n",
      "Gemini-Pro developed by Google DeepMind, is\n",
      "a state-of-the-art LLM with enhanced capabilities\n",
      "for Natural Language Generation and Understanding\n",
      "tasks. It excels in complex reasoning thereby gener-\n",
      "ating nuanced and coherent movie scripts tailored to\n",
      "the specifications. With the inclusion of RAG, Gemini-\n",
      "Pro’s potential hallucination is successfully mitigated.\n",
      "Gemini-Pro has the ability to use either decoder-\n",
      "only or encoder-decoder architecture, based on the\n",
      "structure and type of task, making it a versatile LLM.\n",
      "C. Fine-Tuning LLMs\n",
      "Fine-tuning Large Language Models involves using\n",
      "a pre-trained model and down-streaming it to a specific\n",
      "task like script generation by training it on the task-\n",
      "specific dataset. This process modifies the weights of\n",
      "the pre-trained model to align the predictions accord-\n",
      "ing to the specific task.\n",
      "Using the embeddings of these pre-trained mod-\n",
      "els ensures that the semantic relationships within the\n",
      "movie scripts are captured, so as to generate a proper\n",
      "dialogue flow and adhere to specifications.\n",
      "\n",
      "Fig. 3: Fine-Tuning LLMs Pipeline\n",
      "1) GPT-2\n",
      "GPT-2 (Generative Pre-trained Transformer 2) [14]\n",
      "is a state-of-the-art language model based on the trans-\n",
      "formers architecture developed by OpenAI. It consists\n",
      "of multiple self attention mechanisms and feed forward\n",
      "neural networks to generate contextually relevant and\n",
      "coherent text. GPT-2 is a pre-trained model used for\n",
      "a variety of NLP tasks and thus fine tuning it on a\n",
      "downstream task like text generation would achieve\n",
      "great results. The GPT-2 model used in this paper has\n",
      "approximately 124 million parameters (GPT-2 Small).\n",
      "GPT-2 uses its own tokenizer, as other basic tok-\n",
      "enizers are unable to capture the contextual meaning\n",
      "behind the dialogues and scene settings in a script.\n",
      "It generates tokens based on the contextual under-\n",
      "standing of the sequences, thus capturing the relation\n",
      "between different sequences.\n",
      "It tokenizes words into individual characters and\n",
      "makes character pairs based on consecutive characters.\n",
      "Then it combines two characters into the same token\n",
      "based on the frequency of the repeating character pairs.\n",
      "This process is repeated until a predefined vocabulary\n",
      "size is reached, efficiently and deterministically form-\n",
      "ing a set of subword units which represent the text\n",
      "corpus.\n",
      "The GPT-2 tokenizer has a limitation of only allow-\n",
      "ing a maximum length of 1024 tokens to be encoded.\n",
      "On average the GPT-2 tokenizer makes approximately\n",
      "4 tokens for every 3 words, therefore the scripts get\n",
      "chunked into 700 word chunks and then tokenized.\n",
      "The model is then trained on these individual chunks,\n",
      "retaining all information from the entire script.\n",
      "Once the tokenization is complete, it is ready to be\n",
      "fit into the model for fine-tuning. This approach allows\n",
      "the model to fine-tune on scripts regardless of their size\n",
      "and helps in retaining context as it trains the model on\n",
      "the largest possible chunks, learning patterns between\n",
      "words in a larger sequence.\n",
      "2) Bloom\n",
      "Bloom (BigScience Large Open-science Open-\n",
      "access Multilingual) [15] is another state-of-the-art\n",
      "language model based on the transformers architecture.\n",
      "Similar to GPT-2, it also consists of multiple self at-\n",
      "tention mechanisms and feed forward neural networks\n",
      "to generate contextually relevant and coherent text.\n",
      "Bloom is a pre-trained model which is trained on mul-\n",
      "tilingual data. It has 560 million parameters (Bloom\n",
      "560m) therefore can still prove to have significantly\n",
      "positive results for generating text in a single language.\n",
      "Bloom also uses its own tokenizer which tokenizes\n",
      "text based on a probabilistic approach which is op-\n",
      "timized for diverse scripts and has a more flexible\n",
      "tokenization approach and allows better capturing of\n",
      "context and relationships of words in diverse datasets.\n",
      "It uses SentencePiece which uses a uni-gram language\n",
      "model to build its vocabulary.\n",
      "SentencePiece iteratively learns tokens that are most\n",
      "likely to occur in the training data. It merges the most\n",
      "frequently occurring token pairs into larger tokens and\n",
      "probabilistically estimates the likelihood of merging\n",
      "pairs based on observed frequency distributions of\n",
      "merged tokens. This forms a set of subword units\n",
      "which represent the text corpus.\n",
      "The Bloom tokenizer also has a maximum length of\n",
      "2048 tokens to be encoded. Chunking is also used here\n",
      "to chunk a maximum of 512 tokens per chunk. This\n",
      "allowed for a more efficient tokenization process due\n",
      "to the parallelization of the task using a GPU. After\n",
      "the tokenization, the tokens are ready to fit into the\n",
      "Bloom model for fine-tuning.\n",
      "Since LLM is only using a small percentage of the\n",
      "total parameters, there is loss of accuracy in the script\n",
      "generation, the key is to find the trade-off between\n",
      "the loss of accuracy and reducing computational cost\n",
      "which would make this approach scalable and efficient\n",
      "for large and complex datasets.\n",
      "\n",
      "D. Text-Image Generation\n",
      "Text-Image generation would allow users to visual-\n",
      "ize the scripts through scenes, and hence ideate upon\n",
      "the movie script generation in a robust manner. Text-\n",
      "Image generation begins with tokenizing and creating\n",
      "an embedding of the query to capture the semantic\n",
      "meaning and context of the query.\n",
      "Fig. 4: Working of Stable Diffusion for Text-Image\n",
      "Generation\n",
      "Tokenization creates tokens which are smaller com-\n",
      "ponents of the input query and the embedding is a\n",
      "numerical vector which represents the tokens. Stable\n",
      "Diffusion [16], one of the best algorithms for image\n",
      "generation, begins with random noise and progres-\n",
      "sively learns to refine the noisy image by de-noising\n",
      "it and eventually generating a clear image relevant\n",
      "to the query. It operates within a latent space - a\n",
      "compressed space in which image generation can be\n",
      "done efficiently.\n",
      "Stable Diffusion uses a process called de-noising\n",
      "diffusion which refines the noisy latent representation\n",
      "in a step-by-step process to create a meaningful image.\n",
      "The model aligns the patterns of the image with that\n",
      "of the embedding to form an image which matches the\n",
      "semantical meaning of the given query. Once the image\n",
      "de-noising is performed, a decoder model is used to\n",
      "upsample this image and transform it into a visible\n",
      "image.\n",
      "The algorithm makes use of cross-attention layers\n",
      "which allows the model to focus on different parts\n",
      "of the query embedding at various stages of the de-\n",
      "noising process. This helps in refining the image in\n",
      "order to maintain the coherence and semantic integrity\n",
      "of the input text during the generation of the image.\n",
      "The Stable Diffusion model used for this approach\n",
      "is CompVis.\n",
      "IV. E VALUATION\n",
      "1) Cosine Similarity:\n",
      "Cosine Similarity is a popular metric for assess-\n",
      "ing how semantically similar a generated script\n",
      "is to its input prompt. In a multidimensional\n",
      "space, it is calculated as the cosine of the angle\n",
      "formed by two vectors. The input prompt and the\n",
      "resulting script’s text are represented by these\n",
      "vectors, respectively.\n",
      "The range of a cosine similarity value is -1 to\n",
      "1. A value of 1 denotes complete alignment, in\n",
      "which the output script’s semantic meaning is\n",
      "exactly the same as that of the input prompt.\n",
      "Being orthogonal, a value of 0 indicates no\n",
      "semantic resemblance. A value of -1, on the\n",
      "other hand, indicates total dissimilarity, with the\n",
      "vectors pointing in opposing directions.\n",
      "The model’s ability to preserve semantic co-\n",
      "herence between the input and the generated\n",
      "output improves with increasing cosine similar-\n",
      "ity. When guaranteeing semantic consistency in\n",
      "text creation is a primary goal, this metric is\n",
      "essential.\n",
      "Cosine Similarity(A, B) = A · B\n",
      "∥A∥∥B∥ (8)\n",
      "where, A and B are vectors.\n",
      "2) Perplexity:\n",
      "Perplexity is another crucial assessment metric ,\n",
      "which is mainly applied to text generation tasks.\n",
      "It evaluates a probabilistic model’s ability to\n",
      "anticipate a word sequence. The entropy of the\n",
      "anticipated probability is used to calculate the\n",
      "perplexity value, which goes from 1 to infinity.\n",
      "The model is more certain of its predictions for\n",
      "the subsequent token in the sequence when the\n",
      "perplexity score is lower. On the other hand,\n",
      "greater uncertainty is indicated by a larger per-\n",
      "plexity, which exposes flaws in the probability\n",
      "distribution of the model. A diagnostic method\n",
      "for assessing a language model’s dependability\n",
      "and fluency is perplexity.\n",
      "Low perplexity is a crucial parameter for as-\n",
      "sessing performance in natural language genera-\n",
      "tion tasks since models with low perplexity are\n",
      "generally better able to generate coherent and\n",
      "contextually relevant sequences. Mathematically,\n",
      "Perplexity (PPL) is calculated as:\n",
      "PPL = exp\n",
      " \n",
      "− 1\n",
      "N\n",
      "NX\n",
      "i=1\n",
      "log P(wi | w1, . . . , wi−1)\n",
      "!\n",
      "(9)\n",
      "where, P(wi | w1, . . . , wi−1) is the conditional\n",
      "probability of the i-th word given all the previous\n",
      "words w1, . . . , wi−1 in the sequence and N is\n",
      "the total number of words in the test sequence.\n",
      "3) CLIP Score:\n",
      "It is a metric used to assess how well created\n",
      "\n",
      "words w1, . . . , wi−1 in the sequence and N is\n",
      "the total number of words in the test sequence.\n",
      "3) CLIP Score:\n",
      "It is a metric used to assess how well created\n",
      "images match the written prompts that go with\n",
      "them. CLIP generates vector representations for\n",
      "the created image and the text prompt using\n",
      "a shared embedding space. Cosine similarity is\n",
      "\n",
      "then used to calculate how similar these embed-\n",
      "dings are to one another.\n",
      "While a lower score signifies poor alignment,\n",
      "a higher CLIP Score shows that the resulting\n",
      "image closely reflects the text prompt’s semantic\n",
      "intent. In multi-modal applications where pre-\n",
      "serving the prompt’s integrity in the visual rep-\n",
      "resentation is crucial, such as picture synthesis\n",
      "from textual descriptions, this measure is very\n",
      "useful.\n",
      "The CLIP Score guarantees that produced out-\n",
      "puts satisfy the required degree of semantic\n",
      "accuracy by offering a measurable indicator of\n",
      "textual and visual coherence.\n",
      "CLIP(T, I) = ET · EI\n",
      "∥ET ∥∥EI∥ (10)\n",
      "where, ET and EI are the embeddings of the\n",
      "text and image, respectively and ∥ET ∥ and ∥EI∥\n",
      "represent the norms (magnitudes) of these em-\n",
      "bedding vectors.\n",
      "V. R ESULTS\n",
      "TABLE I: Model evaluation metrics and their average\n",
      "scores\n",
      "Model Metric Score (Average)\n",
      "GPT-2 Cosine Similarity 0.5011\n",
      "GPT-2 Perplexity 1.7443\n",
      "Bloom Cosine Similarity 0.5058\n",
      "Bloom Perplexity 1.6892\n",
      "Gemini-Pro Cosine Similarity 0.5713\n",
      "CompVis CLIP Score 0.3061\n",
      "1) GPT-2 (Cosine Similarity):\n",
      "Fig.5 shows a density plot of cosine similarity\n",
      "using the fine-tuned GPT-2 model. The values\n",
      "go as high as 0.8 and as low as 0.2. The graph\n",
      "suggests that the generated script at times has\n",
      "very high contextual similarity between the input\n",
      "query and the generated script and at times very\n",
      "low.\n",
      "Fig. 5: Density Plot for Cosine Similarity (fine-tuned\n",
      "GPT-2)\n",
      "The lower values can suggest poor generation\n",
      "leading to lesser context being included in the\n",
      "generated script or can be due to high creativity\n",
      "which leads to a variety of information along\n",
      "with the details outlined in the prompt being\n",
      "mentioned in the script.\n",
      "The higher values suggest that the script per-\n",
      "fectly encompasses the required details from the\n",
      "prompt but can also suggest that the generation\n",
      "had lesser creativity and did not add many details\n",
      "which were not mentioned. This can suggest\n",
      "that a great input prompt with a lot of details\n",
      "was mentioned or the generation was with poor\n",
      "creativity.\n",
      "Majority of the cosine similarity values lying\n",
      "near the average of 0.5011 suggests that the\n",
      "approach was able to generate a script which\n",
      "contained the details from the prompt and was\n",
      "also able to include creative aspects in the script.\n",
      "2) GPT-2 (Perplexity):\n",
      "Fig.6 shows a density plot of the perplexity using\n",
      "fine-tuned GPT-2. The perplexity had an average\n",
      "of 1.7443. The perplexity value varied on the\n",
      "lower side and upper side of this average value.\n",
      "The lower values suggest that the model was\n",
      "less surprised in the token generation of these\n",
      "scripts leading to a well structured script where\n",
      "followup words were expected more often than\n",
      "not.\n",
      "Fig. 6: Density Plot for Perplexity (GPT-2)\n",
      "The higher values can signify a script which\n",
      "made the model more surprised during the token\n",
      "generation. This suggests that the grammatical\n",
      "and semantic clarity of the sentences was jeopar-\n",
      "dized and the sentences more often than not, did\n",
      "not make sense grammatically and contextually.\n",
      "The scripts with perplexity near the average\n",
      "value can suggest that they made sense gram-\n",
      "matically and contextually to some degree but\n",
      "could have either left something to be desired\n",
      "in the generation or the creative aspect of the\n",
      "generation left the model more surprised at the\n",
      "token generation, attributing to its slightly higher\n",
      "perplexity score. This could suggest that the\n",
      "ideal value of perplexity for script generation\n",
      "which includes good creativity could be that with\n",
      "values near the average perplexity of 1.7443.\n",
      "3) Bloom (Cosine Similarity):\n",
      "\n",
      "Fig.7 shows a density plot of cosine similar-\n",
      "ity using the fine-tuned Bloom model, which\n",
      "differs from the fine-tuned GPT-2 model in its\n",
      "distribution and average values. While the cosine\n",
      "similarity for GPT-2 averaged at 0.5011, the\n",
      "Bloom model’s average was 0.5058.\n",
      "Fig. 7: Density Plot for Cosine Similarity (fine-tuned\n",
      "Bloom)\n",
      "Both models showed a range of values from\n",
      "0.2 to 0.8, indicating varying levels of con-\n",
      "textual similarity between the input query and\n",
      "the generated script. However, the Bloom model\n",
      "appears to strike a slightly different balance, with\n",
      "its clustering around the average suggesting a\n",
      "consistent ability to include prompt details while\n",
      "introducing creative aspects.\n",
      "In comparison, the GPT-2 model’s lower and\n",
      "higher values might more distinctly reflect out-\n",
      "puts that either deviate significantly from the\n",
      "prompt or align too closely, potentially at the\n",
      "expense of creativity. The differences in average\n",
      "values and distributions suggest that Bloom may\n",
      "provide a more balanced generation, while GPT-\n",
      "2 may exhibit greater variance in its outputs\n",
      "depending on the nature of the prompt.\n",
      "According to the variations in average values\n",
      "and distributions, Bloom might produce a more\n",
      "balanced generation, but GPT-2 might produce\n",
      "outputs that vary more depending on the prompt.\n",
      "4) Bloom (Perplexity):\n",
      "Fig.6 compares the density plots of perplexity\n",
      "for fine-tuned GPT-2 and Bloom models, demon-\n",
      "strating variations in their text generation. GPT-2\n",
      "recorded an average perplexity of 1.7443, while\n",
      "Bloom achieved a lesser average of 1.6892.\n",
      "Fig. 8: Density Plot for Perplexity (fine-tuned Bloom)\n",
      "Higher perplexity values for GPT-2 indicate dif-\n",
      "ficulties in preserving grammatical or semantic\n",
      "clarity, which frequently results in outputs that\n",
      "lack contextual sense. Lower perplexity val-\n",
      "ues, on the other hand, indicate well-structured\n",
      "scripts with predictable token generation. Al-\n",
      "though some may have had space for growth in\n",
      "generation quality, scripts with ambiguity close\n",
      "to the average demonstrated a balance between\n",
      "coherence and inventiveness.\n",
      "Bloom’s tighter grouping and lower confusion,\n",
      "on the other hand, point to a more reliable\n",
      "capacity for producing language that is both con-\n",
      "textually relevant and coherent. Its values close\n",
      "to the mean show a steady equilibrium between\n",
      "following the prompt and adding originality.\n",
      "Bloom’s smaller range of perplexity scores sug-\n",
      "gests a lower variability than GPT-2, which\n",
      "makes it more dependable for applications de-\n",
      "manding accuracy. However, applications that\n",
      "value diversity and flexibility in script produc-\n",
      "tion could find GPT-2 more appropriate due to\n",
      "its wider range\n",
      "5) Gemini-Pro (Cosine Similarity):\n",
      "Fig.9 shows the density plot shows the distri-\n",
      "bution of cosine similarity values for outputs\n",
      "produced by the Gemini-Pro model using RAG,\n",
      "with an average cosine similarity of 0.5713.The\n",
      "range is from 0.2 to 0.8.\n",
      "This variability shows how well the model per-\n",
      "forms in striking a balance between contex-\n",
      "tual alignment of generated scripts and creative\n",
      "flexibility; lower cosine similarity values, closer\n",
      "to 0.2, indicate instances where the generated\n",
      "scripts either lack sufficient context from the in-\n",
      "put prompt or exhibit high creativity by deviating\n",
      "from the input to include diverse information;\n",
      "higher values, approaching 0.8, indicate outputs\n",
      "that closely align with the prompt, effectively\n",
      "capturing its details; however, such outputs may\n",
      "also suggest limited creativity, as the generation\n",
      "may strictly follow the prompt without adding\n",
      "subtle or unique information.\n",
      "Fig. 9: Density Plot for Cosine Similarity (Gemini-\n",
      "Pro)\n",
      "The concentration of cosine similarity values\n",
      "around the average of 0.5713 shows that Gemini-\n",
      "\n",
      "Pro successfully strikes a balance between cre-\n",
      "ativity and relevance, generating scripts that use\n",
      "RAG to add meaningful and creative elabora-\n",
      "tion to the content while incorporating relevant\n",
      "information from the prompt.This performance\n",
      "shows how well Gemini-Pro combines creative\n",
      "flexibility with rapid alignment, outperforming\n",
      "other models in the process.\n",
      "6) CompVis (CLIP Score):\n",
      "Fig.10 shows a density plot for the CLIP score\n",
      "of the image generated from the input prompt.\n",
      "The average CLIP score was 0.3061.\n",
      "This shows a weaker alignment of the image\n",
      "with the features mentioned in the prompt or\n",
      "could also suggest that the features in the image\n",
      "are loosely related to the details in the prompt.\n",
      "This could be due to the model under-performing\n",
      "or the input prompts not being detailed enough\n",
      "to generate distinct features in the image.\n",
      "Fig. 10: Density Plot for CLIP Score (CompVis)\n",
      "Aside from lack of details in the prompt, abstract\n",
      "or complex details could also lead to a lower\n",
      "CLIP score since the model might be unable to\n",
      "make a strong connection between these con-\n",
      "cepts and features in the image.\n",
      "7) Reason why Gemini-Pro cannot be evalu-\n",
      "ated using Perplexity: To calculate perplexity,\n",
      "a probabilistic token-based prediction approach\n",
      "is required but since Gemini-Pro is being inte-\n",
      "grated with the RAG Pipeline, it is not using\n",
      "any probability distribution to predict the next\n",
      "tokens, it rather combines the retrieval mecha-\n",
      "nisms of RAG with generation.\n",
      "Since RAG models do not use a probabilistic\n",
      "approach to predict the next tokens, Gemini-Pro\n",
      "thus does not produce a probability distribution\n",
      "and perplexity cannot be used as an evaluation\n",
      "metric for Gemini-Pro.\n",
      "VI. F UTURE SCOPE\n",
      "Due to hardware constraints, the LLMs used in the\n",
      "proposed methodology have limited capabilities. Al-\n",
      "though LLMs provided accurate responses to the query\n",
      "and performed well on the evaluation metrics, better\n",
      "models with a greater number of parameters can be\n",
      "used to further refine the movie scripts to adhere to the\n",
      "specifications in a better way. Better stable diffusion\n",
      "models could also be used to generate accurate text\n",
      "inside an image, or generate higher quality images for\n",
      "better visualization of the generated movie script.\n",
      "The models could also be optimized to give sug-\n",
      "gestions for what kind of lighting or scene setting is\n",
      "needed to direct such a movie script in an apt manner\n",
      "by storing more metadata about the script in the vector\n",
      "database.\n",
      "VII. C ONCLUSION\n",
      "The proposed methodologies demonstrate the poten-\n",
      "tial of advanced NLP techniques like RAG and Fine-\n",
      "Tuning LLMs, and Image Generation algorithms like\n",
      "Stable Diffusion in order to automate script and scene\n",
      "generation in the cinema industry.\n",
      "The lower perplexity and high cosine similarity\n",
      "values indicate a high level of coherence and fluency\n",
      "in generated dialogues, which is important for main-\n",
      "taining human-like script writing techniques. CLIP\n",
      "Score indicates that despite the room for improvement,\n",
      "Stable Diffusion can help visualize the input query in\n",
      "the form of a movie scene with a moderate level of\n",
      "alignment.\n",
      "This approach reduces the barriers in creative pro-\n",
      "cesses, allowing a better overall script generation.\n",
      "ACKNOWLEDGMENT\n",
      "We would like to extend our sincere gratitude to\n",
      "Symbiosis Institute of Technology, Symbiosis Interna-\n",
      "tional (Deemed University), Pune, India, for providing\n",
      "the invaluable support and platform necessary to con-\n",
      "duct our research work. We are thankful for providing\n",
      "the nurturing environment where we had the privilege\n",
      "to learn and explore.\n",
      "REFERENCES\n",
      "[1] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Eval-\n",
      "uation of text generation: A survey. arXiv preprint\n",
      "arXiv:2006.14799, 2020.\n",
      "[2] A Vaswani. Attention is all you need. Advances in Neural\n",
      "Information Processing Systems , 2017.\n",
      "[3] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,\n",
      "and Tatsunori B Hashimoto. Diffusion-lm improves con-\n",
      "trollable text generation. Advances in Neural Information\n",
      "\n",
      "[3] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,\n",
      "and Tatsunori B Hashimoto. Diffusion-lm improves con-\n",
      "trollable text generation. Advances in Neural Information\n",
      "Processing Systems, 35:4328–4343, 2022.\n",
      "[4] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical\n",
      "neural story generation. arXiv preprint arXiv:1805.04833 ,\n",
      "2018.\n",
      "[5] Tom B Brown. Language models are few-shot learners. arXiv\n",
      "preprint arXiv:2005.14165, 2020.\n",
      "[6] Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J Martin,\n",
      "Animesh Mehta, Brent Harrison, and Mark O Riedl. Control-\n",
      "lable neural story plot generation via reward shaping. arXiv\n",
      "preprint arXiv:1809.10736, 2018.\n",
      "[7] Sanidhya Mangal, Poorva Joshi, and Rahul Modak. Lstm vs.\n",
      "gru vs. bidirectional rnn for script generation. arXiv preprint\n",
      "arXiv:1908.04332, 2019.\n",
      "[8] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight,\n",
      "Dongyan Zhao, and Rui Yan. Plan-and-write: Towards better\n",
      "automatic storytelling. In Proceedings of the AAAI Conference\n",
      "on Artificial Intelligence , volume 33, pages 7378–7385, 2019.\n",
      "[9] Yutao Zhu, Ruihua Song, Zhicheng Dou, Jian-Yun Nie, and Jin\n",
      "Zhou. Scriptwriter: Narrative-guided script generation. arXiv\n",
      "preprint arXiv:2005.10331, 2020.\n",
      "\n",
      "[10] Gayatri Bhat, Avneesh Saluja, Melody Dye, and Jan Flor-\n",
      "janczyk. Hierarchical encoders for modeling and interpreting\n",
      "screenplays. arXiv preprint arXiv:2004.14532 , 2020.\n",
      "[11] Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong,\n",
      "and Changyou Chen. Outline to story: Fine-grained control-\n",
      "lable story generation from cascaded events. arXiv preprint\n",
      "arXiv:2101.00822, 2021.\n",
      "[12] Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras,\n",
      "Niket Tandon, Peter Clark, and Yejin Choi. proscript: Partially\n",
      "ordered scripts generation via pre-trained language models.\n",
      "arXiv preprint arXiv:2104.08251 , 2021.\n",
      "[13] Fatima Dayo, Ahmed Ali Memon, and Nasrullah Dharejo.\n",
      "Scriptwriting in the age of ai: Revolutionizing storytelling with\n",
      "artificial intelligence. Journal of Media & Communication ,\n",
      "4(1):24–38, 2023.\n",
      "[14] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\n",
      "Amodei, Ilya Sutskever, et al. Language models are unsuper-\n",
      "vised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "[15] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,\n",
      "Suzana Ili ´c, Daniel Hesslow, Roman Castagn ´e, Alexan-\n",
      "dra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e, et al.\n",
      "Bloom: A 176b-parameter open-access multilingual language\n",
      "model. 2023.\n",
      "[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\n",
      "Esser, and Bj ¨orn Ommer. High-resolution image synthesis\n",
      "with latent diffusion models. In Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition , pages\n",
      "10684–10695, 2022.\n",
      "[17] Y Zhang. Dialogpt: Large-scale generative pre-training\n",
      "for conversational response generation. arXiv preprint\n",
      "arXiv:1911.00536, 2019.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01jk3me1zrffhs062c25kkqg8s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10074, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:606\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    607\u001b[0m         _output_key\n\u001b[0;32m    608\u001b[0m     ]\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    612\u001b[0m         _output_key\n\u001b[0;32m    613\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:259\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:474\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    470\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    473\u001b[0m }\n\u001b[1;32m--> 474\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:298\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1263\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1251\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1260\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1261\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1262\u001b[0m     )\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\groq\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01jk3me1zrffhs062c25kkqg8s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10074, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "summary = chain.run(docs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
