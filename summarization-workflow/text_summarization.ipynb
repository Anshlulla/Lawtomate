{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIMessage, SystemMessage, HumanMessage\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002C36BBBF500>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002C36D5288F0>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama3-8b-8192\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Ladies and gentlemen, thank you for being here today. We are gathered to celebrate the achievements of our team over the past year. \n",
    "Our hard work and dedication have led to remarkable success, and I am incredibly proud of what we have accomplished together. \n",
    "As we look to the future, let us continue to strive for excellence and innovation. Together, we can achieve even greater heights. \n",
    "Thank you for your commitment and passion. Let's make the coming year even more successful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message = [\n",
    "    SystemMessage(content=\"You are an expert with expertise in summarizing text.\"),\n",
    "    HumanMessage(content=f\"Please provide a concise summary of the following text:\\n {text}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Ansh Lulla\\VS-Code\\Langchain_tutorial\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ansh Lulla\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker congratulates the team on their remarkable achievements over the past year, expressing pride in their hard work and dedication. They encourage the team to strive for excellence and innovation in the future, emphasizing the potential for even greater success if they continue to work together.\n"
     ]
    }
   ],
   "source": [
    "summary = llm(chat_message).content\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='\\n    Write a summary based on the following text:\\n    Text: {text}\\n    Translate the precise summary to {language}\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    Write a summary based on the following text:\n",
    "    Text: {text}\n",
    "    Translate the precise summary to {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"text\", \"language\"], template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Write a summary based on the following text:\n",
      "    Text: \n",
      "Ladies and gentlemen, thank you for being here today. We are gathered to celebrate the achievements of our team over the past year. \n",
      "Our hard work and dedication have led to remarkable success, and I am incredibly proud of what we have accomplished together. \n",
      "As we look to the future, let us continue to strive for excellence and innovation. Together, we can achieve even greater heights. \n",
      "Thank you for your commitment and passion. Let's make the coming year even more successful.\n",
      "\n",
      "    Translate the precise summary to Hindi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_prompt = prompt.format(text=text, language=\"Hindi\")\n",
    "print(complete_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(complete_prompt) # also considers the prompt template content written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='\\n    Write a summary based on the following text:\\n    Text: {text}\\n    Translate the precise summary to {language}\\n'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002C36BBBF500>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002C36D5288F0>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Write a summary based on the following text:\n",
      "    Text: \n",
      "Ladies and gentlemen, thank you for being here today. We are gathered to celebrate the achievements of our team over the past year. \n",
      "Our hard work and dedication have led to remarkable success, and I am incredibly proud of what we have accomplished together. \n",
      "As we look to the future, let us continue to strive for excellence and innovation. Together, we can achieve even greater heights. \n",
      "Thank you for your commitment and passion. Let's make the coming year even more successful.\n",
      "\n",
      "    Translate the precise summary to Marathi\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here is a precise summary of the text:\n",
      "\n",
      "\"We celebrate our team's achievements over the past year, acknowledging our hard work and dedication. We look forward to continued excellence and innovation, striving to achieve even greater heights together. We thank our team members for their commitment and passion, and look to make the upcoming year even more successful.\"\n",
      "\n",
      "And here is the translation of the summary to Marathi:\n",
      "\n",
      "\"आमच्या टीमच्या पापोळा वर्षानंतर प्राप्त झालेल्या साधारण सफ़लतेस स्वागत करतो. आमच्या कठिन कार्य आणि निष्ठा आहे ज्यामुळे आमच्या पुढील प्रयासातील उत्कृष्टता आणि नवीनता निर्माण करतील. आमच्या टीम सदस्यांना त्यांच्या देखलेल्या संकल्प आणि स्नेहसाठी धन्यवाद. पुढील वर्षातही अधिक सफ़ल्य प्राप्त करण्यासाठी!\"\n"
     ]
    }
   ],
   "source": [
    "summary = chain.run({\"text\": text, \"language\": \"Marathi\"})\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
